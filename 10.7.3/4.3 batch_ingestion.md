# Machine Learning Batch Ingestion Tutorial
Using externally hosted ML models for batch ingestion. The `text_embedding` and `sparse_encoding` processors currently support batch ingestion.
## Steps

1. **Register a model group**
2. **Create a connector**
3. **Register an externally hosted model**
4. **Deploy the model**
5. **Create an ingest pipeline**
6. **Perform bulk indexing**

## Asynchronous batch ingestion
Batch ingestion configures an ingest pipeline, which processes documents one by one. For each document, batch ingestion calls an externally hosted model to generate text embeddings from the document text and then ingests the document, including text and embeddings, into an OpenSearch index.
An alternative to this real-time process, asynchronous batch ingestion, ingests both documents and their embeddings generated outside of OpenSearch and stored on a remote file server, such as Amazon Simple Storage Service (Amazon S3) or OpenAI. Asynchronous ingestion returns a task ID and runs asynchronously to ingest data offline into your k-NN cluster for neural search. You can use asynchronous batch ingestion together with the Batch Predict API to perform inference asynchronously.

[Read more](https://opensearch.org/docs/latest/ml-commons-plugin/remote-models/async-batch-ingestion/)