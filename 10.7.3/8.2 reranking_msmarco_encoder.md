# Reranking search results using the MS MARCO cross-encoder model
A reranking pipeline can rerank search results, providing a relevance score for each document in the search results with respect to the search query. The relevance score is calculated by a cross-encoder model.

This tutorial illustrates how to use the Hugging Face ms-marco-MiniLM-L-6-v2 model in a reranking pipeline.

## Step 1: Create the OpenAI Connector

First, create a connector for the OpenAI model:

```json
POST /_plugins/_ml/connectors/_create
{
    "name": "OpenAI GPT-3.5 Connector",
    "description": "Connector to OpenAI GPT-3.5 model",
    "version": 1,
    "protocol": "http",
    "parameters": {
        "endpoint": "api.openai.com",
        "model": "gpt-3.5-turbo"
    },
    "credential": {
        "openai_key": "your_openai_api_key_here"
    },
    "actions": [
        {
            "action_type": "predict",
            "method": "POST",
            "url": "https://${parameters.endpoint}/v1/chat/completions",
            "headers": {
                "Authorization": "Bearer ${credential.openai_key}",
                "Content-Type": "application/json"
            },
            "request_body": "{ \"model\": \"${parameters.model}\", \"messages\": ${parameters.messages} }"
        }
    ]
}
```

## Step 2: Register the Model

Use the connector ID from the previous step to register the model:

```json
POST /_plugins/_ml/models/_register
{
    "name": "OpenAI GPT-3.5 Model",
    "function_name": "remote",
    "description": "OpenAI GPT-3.5 model for text generation",
    "connector_id": "your_connector_id_here"
}
```

## Step 3: Deploy the Model

Deploy the registered model:

```json
POST /_plugins/_ml/models/your_model_id_here/_deploy
```

## Step 4: Create a RAG Agent

Create a Retrieval-Augmented Generation (RAG) agent using the deployed model:

```json
POST /_plugins/_ml/agents/_register
{
    "name": "OpenAI RAG Agent",
    "type": "conversational_flow",
    "description": "RAG agent using OpenAI GPT-3.5",
    "memory": {
        "type": "conversation_index"
    },
    "tools": [
        {
            "type": "VectorDBTool",
            "name": "knowledge_base",
            "parameters": {
                "index": "your_vector_index_name",
                "embedding_field": "embedding",
                "source_field": ["text"],
                "input": "${parameters.question}"
            }
        },
        {
            "type": "MLModelTool",
            "name": "openai_gpt_model",
            "description": "OpenAI GPT-3.5 model for text generation",
            "parameters": {
                "model_id": "your_model_id_here",
                "prompt": "Answer the question based on the given context:\n\nContext: ${parameters.knowledge_base.output}\n\nQuestion: ${parameters.question}\n\nAnswer:"
            }
        }
    ]
}
```

## Step 5: Execute the Agent

To use the RAG agent, execute it with a question:

```json
POST /_plugins/_ml/agents/your_agent_id_here/_execute
{
    "parameters": {
        "question": "What is OpenSearch?"
    }
}
```