# Configuring model guardrails
Guardrails can guide a large language model (LLM) toward desired behavior. They act as a filter, preventing the LLM from generating output that is harmful or violates ethical principles and facilitating safer use of AI. Guardrails also cause the LLM to produce more focused and relevant output.

You can configure guardrails for your LLM using the following methods:

- Provide a list of words to be prohibited in the input or output of the model. Alternatively, you can provide a regular expression against which the model input or output will be matched. For more information, see [Validating input/output using stopwords and regex](https://opensearch.org/docs/latest/ml-commons-plugin/remote-models/guardrails/#validating-inputoutput-using-stopwords-and-regex).
- Configure a separate LLM whose purpose is to validate the user input and the LLM output.

## Steps to Validate Stopwords in OpenSearch

1. Create a guardrail index
   - Create an index to store excluded words (stopwords)
   - Specify a `title` field for excluded words
   - Include a `query` field of the [percolator type](https://opensearch.org/docs/latest/field-types/supported-field-types/percolator/)

2. Index excluded words or phrases
   - Use query string queries to match excluded words

3. Register a model group
   - Send a POST request to register a model group
   - Obtain the model group ID from the response

4. Create a connector
   - Create a connector for the model (e.g., Anthropic Claude on Amazon Bedrock)
   - Obtain the connector ID from the response

5. Register and deploy the model with guardrails
   - Use the model group ID and connector ID from previous steps
   - Include the `guardrails` object in the request
   - Specify stopwords and regex patterns for input and output validation

6. Test the model (optional)
   - Send predict operations with and without excluded words
   - Verify that guardrails are triggered for inputs containing stopwords